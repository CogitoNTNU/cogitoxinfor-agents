{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An agent from langchain that can analyze images and text\n",
    "\n",
    "Made with inspiration from this source code from langchain:\n",
    "\n",
    "https://python.langchain.com/docs/how_to/document_loader_pdf/#use-of-multimodal-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "# uncomment the following lines to install the necessary libraries\n",
    "\n",
    "# pip install fitz pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import fitz\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def pdf_page_to_base64(pdf_path: str, page_number: int):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    page = pdf_document.load_page(page_number - 1)  # input is one-indexed\n",
    "    pix = page.get_pixmap()\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    img.save(buffer, format=\"PNG\")\n",
    "\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def get_pdf_content(pdf_path: str) -> dict:\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text.append(page.get_text())\n",
    "    return text\n",
    "\n",
    "# Get the content of the PDF file\n",
    "pdf_text = get_pdf_content(\"../../data/test_with_images.pdf\")\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = \"../../data/test_with_images.pdf\"\n",
    "\n",
    "# Initialize the multimodal model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Iterate over each page and analyze content\n",
    "for page_num, page_text in enumerate(pdf_text, start=1):\n",
    "    # Convert the page to a base64 image\n",
    "    base64_image = pdf_page_to_base64(pdf_path, page_num)\n",
    "    display(IPImage(data=base64.b64decode(base64_image)))\n",
    "\n",
    "    # Define the query\n",
    "    query = f\"Analyze the content of page {page_num}\"\n",
    "\n",
    "    # Create the message with text and image\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": query},\n",
    "            {\"type\": \"text\", \"text\": page_text},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    # Query the model\n",
    "    response = llm.invoke([message])\n",
    "    print(f\"Page {page_num} analysis:\\n{response.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
