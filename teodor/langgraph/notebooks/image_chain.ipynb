{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ASTRA_DB_API_KEY = os.getenv('ASTRA_DB_API_KEY')\n",
    "ASTRA_DB_ENDPOINT = os.getenv('ASTRA_DB_ENDPOINT')\n",
    "ASTRA_DB_KEYSPACE = os.getenv('ASTRA_DB_KEYSPACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import globals\n",
    "from langchain_core.runnables import chain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "def load_image(inputs: dict) -> dict:\n",
    "    \"\"\"Load image from file and encode it as base64.\"\"\"\n",
    "    image_path = inputs[\"image_path\"]\n",
    "  \n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    image_base64 = encode_image(image_path)\n",
    "    return {\"image\": image_base64}\n",
    "\n",
    "load_image_chain = TransformChain(\n",
    "    input_variables=[\"image_path\"],\n",
    "    output_variables=[\"image\"],\n",
    "    transform=load_image\n",
    ")\n",
    "\n",
    "\n",
    "class ImageInformation(BaseModel):\n",
    "    \"\"\"Information about an image.\"\"\"\n",
    "    image_description: str = Field(description=\"a short description of the image\")\n",
    "    people_count: int = Field(description=\"number of humans on the picture\")\n",
    "    main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n",
    "    humans: int = Field(description=\"number of humans on the picture\")\n",
    "    \n",
    "     \n",
    "# Set verbose\n",
    "globals.set_debug(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "def image_agent(inputs: dict) -> str | list[str] | dict:\n",
    " \"\"\"Invoke model with image and prompt.\"\"\"\n",
    " model = ChatOpenAI(temperature=0.5, model=\"gpt-4o-mini\", max_tokens=1024)\n",
    " msg = model.invoke(\n",
    "             [HumanMessage(\n",
    "             content=[\n",
    "             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n",
    "             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n",
    "             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n",
    "             ])]\n",
    "             )\n",
    " return msg.content\n",
    "\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=ImageInformation)\n",
    "\n",
    "def get_image_informations(image_path: str) -> dict:\n",
    "   vision_prompt = \"\"\"\n",
    "   Given the image, provide the following information:\n",
    "   - A count of how many people are in the image\n",
    "   - A list of the main objects present in the image\n",
    "   - A description of the image\n",
    "   \"\"\"\n",
    "   vision_chain = load_image_chain | image_agent | parser\n",
    "   return vision_chain.invoke({'image_path': f'{image_path}', \n",
    "                               'prompt': vision_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.5, model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ImageInformation(BaseModel):\n",
    "    \"\"\"Information about an image.\"\"\"\n",
    "    image_description: str = Field(description=\"a short description of the image\")\n",
    "    people_count: int = Field(description=\"number of humans on the picture\")\n",
    "    main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n",
    "    humans: int = Field(description=\"number of humans on the picture\")\n",
    "\n",
    "\n",
    "\n",
    "structured_image_agent = llm.with_structured_output(ImageInformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_image_informations(\"path/to/your/image.jpg\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
