{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdac0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Union, Sequence, List, Any, Optional, Dict, Literal\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage, ToolMessage, AIMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from load_model import load_model\n",
    "import json\n",
    "import operator\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import asyncio\n",
    "from langgraph.types import interrupt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"agent_graph\")\n",
    "\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the agent's predicted action and its arguments.\n",
    "    \"\"\"\n",
    "    action: str = Field(description=\"The action to be performed by the agent\")\n",
    "    args: List[str] = Field(default_factory=list, description=\"Arguments for the action\")\n",
    "\n",
    "\n",
    "class OldState(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the state of the agent during execution.\n",
    "    \"\"\"\n",
    "    testing: bool = Field(default=False, description=\"Flag indicating if the agent is in testing mode\")\n",
    "    test_actions: Optional[List[Prediction]] = Field(default_factory=list, description=\"The action to be tested\")\n",
    "    prediction: Optional[Prediction] = Field(None, description=\"The agent's predicted action and arguments\")\n",
    "    repeated_failures: Optional[int] = Field(default=0, description=\"Count of repeated failures for the current action\")\n",
    "    human_in_the_loop: bool = False\n",
    "    DEBUG: bool = True\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(OldState):\n",
    "    \"\"\"State of the agent.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None] = None\n",
    "    return_direct: bool = False\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add] = Field(default_factory=list)\n",
    "    model_name: str = \"gpt-4.1\"  # Default model name\n",
    "    test_responses: Optional[List[Dict[str, Any]]] = None  # Predefined responses for testing\n",
    "    prompt_template: Optional[ChatPromptTemplate] = None  # Added prompt template field\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "def create_agent_graph(\n",
    "    tools, \n",
    "    checkpointer=None\n",
    "):\n",
    "    \"\"\"Create an async LangGraph REACT agent with customizable prompts and testing capabilities.\n",
    "    \n",
    "    Args:\n",
    "        tools: List of available tools\n",
    "        system_prompt: Optional custom system prompt for formatting instructions\n",
    "        design_template: Optional design template as a human message\n",
    "        checkpointer: Optional checkpointer for state persistence\n",
    "        \n",
    "    Returns:\n",
    "        Compiled async StateGraph for the agent\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating agent graph with {len(tools)} tools\")\n",
    "\n",
    "    @RunnableLambda\n",
    "    async def call_model(state: AgentState, config: RunnableConfig):\n",
    "        \"\"\"Call the LLM with the current conversation state.\"\"\"\n",
    "        logger.info(f\"Calling model {state.model_name} with {len(state.messages)} messages\")\n",
    "        \n",
    "        llm = load_model(\n",
    "            model_name=state.model_name, \n",
    "            tools=tools, \n",
    "            )\n",
    "            \n",
    "        response = await llm.ainvoke(input=state.messages, config=config)\n",
    "        \n",
    "        # Return updated state with the response\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "\n",
    "    @RunnableLambda\n",
    "    async def human_input(state: AgentState, config: RunnableConfig) -> AgentState:\n",
    "        \"\"\"\n",
    "        Conditionally interrupts the graph for human approval based on state.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a descriptive message about what's happening\n",
    "        test_action = state.test_actions[0]\n",
    "        action = test_action.action\n",
    "        args = test_action.args\n",
    "        # Format the message for user interaction\n",
    "        message = f\"Agent wants to perform: {action} with args: {args}\"\n",
    "        \n",
    "        # This will pause execution and return the message to the client\n",
    "        user_input = interrupt(message)\n",
    "        \n",
    "        # When execution resumes, user_input will contain the value\n",
    "        # passed via Command(resume=...)\n",
    "        if user_input == '':\n",
    "            # If user just pressed Enter, keep the original action and args\n",
    "            # No changes needed to state.test_actions[0]\n",
    "            print(f\"User accepts the action: {action} with args: {args}\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"User provided alternative input: {user_input}\")\n",
    "            if user_input == 'exit':\n",
    "                # If the user wants to exit, set the action to \"exit\"\n",
    "                state.test_actions[0] = Prediction(action=\"exit\", args=[])\n",
    "            else:\n",
    "                # Only update if the user provided alternative input\n",
    "                # Convert the input to a list to maintain proper args format\n",
    "                new_args = [user_input]\n",
    "                state.test_actions[0] = Prediction(action=action, args=new_args)\n",
    "\n",
    "        return state\n",
    "\n",
    "    @RunnableLambda\n",
    "    async def process_tool_execution(state: AgentState):\n",
    "        \"\"\"Execute tools and track intermediate steps.\"\"\"\n",
    "        # Get the last message with tool calls\n",
    "        last_message = state.messages[-1]\n",
    "        if not (isinstance(last_message, AIMessage) and last_message.tool_calls):\n",
    "            return state\n",
    "\n",
    "        tool_call = last_message.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_input = tool_call[\"args\"]\n",
    "        \n",
    "        # Log the tool execution\n",
    "        logger.info(f\"Executing tool: {tool_name} with input: {json.dumps(tool_input)[:100]}...\")\n",
    "        \n",
    "        # Create tool node for this execution\n",
    "        tool_node = ToolNode(\n",
    "            tools=tools,\n",
    "            handle_tool_errors=lambda exception, tool_call: (\n",
    "                f\"Error executing tool {tool_call.get('name')}: {str(exception)}\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        result = await tool_node.ainvoke(state)\n",
    "        \n",
    "        # Create the agent action record\n",
    "        agent_action = AgentAction(\n",
    "            tool=tool_name,\n",
    "            tool_input=tool_input,\n",
    "            log=last_message.content\n",
    "        )\n",
    "        \n",
    "        # Get the tool message content\n",
    "        tool_message = result[\"messages\"][-1]\n",
    "        \n",
    "        # Add to intermediate steps\n",
    "        new_steps = state.intermediate_steps + [(agent_action, tool_message.content)]\n",
    "        \n",
    "        logger.info(f\"New intermediate steps: {len(new_steps)}\")\n",
    "        \n",
    "        # Return updated state\n",
    "        return {\n",
    "            \"messages\": result[\"messages\"],\n",
    "            \"intermediate_steps\": new_steps\n",
    "        }\n",
    "\n",
    "    # Create the state graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    logger.info(\"Initializing state graph\")\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"agent\", call_model)\n",
    "    workflow.add_node(\"tools\", process_tool_execution)\n",
    "    workflow.add_node(\"human_input_node\", human_input)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    \n",
    "    # Define conditional edge routing\n",
    "    def should_continue(state: AgentState) -> str:\n",
    "        \"\"\"Determine if we should continue with tools or end the conversation.\"\"\"\n",
    "        last_message = state.messages[-1]\n",
    "        \n",
    "        if state.human_in_the_loop and isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            # Extract tool call information\n",
    "            tool_call = last_message.tool_calls[0]\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            \n",
    "            # Add to test_actions\n",
    "            state.test_actions = [Prediction(action=tool_name, args=[json.dumps(tool_args)])]\n",
    "            \n",
    "            logger.info(\"Decision: Human intervention required\")\n",
    "            return \"human_input_node\"\n",
    "        \n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            logger.info(\"Decision: Agent requested tool execution\")\n",
    "            return \"tools\"\n",
    "        else:\n",
    "            logger.info(\"Decision: Agent completed task, ending workflow\")\n",
    "            return \"end\"\n",
    "    \n",
    "    # Connect nodes\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"tools\": \"tools\",\n",
    "            \"end\": END,\n",
    "            \"human_input_node\": \"human_input_node\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    def should_resume(state: AgentState) -> str:\n",
    "        \"\"\"Determine if we should resume the workflow after human input.\"\"\"\n",
    "        action = state.test_actions[0].action\n",
    "        if action == \"exit\":\n",
    "            logger.info(\"Decision: User requested exit\")\n",
    "            return \"end\"\n",
    "        elif action == \"new_action\":\n",
    "            logger.info(\"Decision: User requested new action\")\n",
    "            return \"agent\"\n",
    "        else:\n",
    "            logger.info(\"Decision: User wants to continue with action\")\n",
    "            return \"tools\"\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_input_node\",\n",
    "        should_resume,\n",
    "        {\n",
    "            \"agent\": \"agent\",\n",
    "            \"tools\": \"tools\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Graph structure defined and edges connected\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    if checkpointer:\n",
    "        logger.info(\"Compiling graph with checkpointer\")\n",
    "        return workflow.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    logger.info(\"Compiling graph without checkpointer\")\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc384f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:agent_graph:Creating agent graph with 0 tools\n",
      "INFO:agent_graph:Initializing state graph\n",
      "INFO:agent_graph:Graph structure defined and edges connected\n",
      "INFO:agent_graph:Compiling graph without checkpointer\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "graph = create_agent_graph(tools=[])\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional # Added Any, Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import logging # Add logging import\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ConfigLLM(BaseModel):\n",
    "    model_name: str = os.getenv(\"LLM_MODEL_NAME\", \"gpt-4.1-nano\")\n",
    "    temperature: float = 0.0\n",
    "\n",
    "class MCPAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ConfigLLM()\n",
    "        \n",
    "        self.mcp_servers = {\n",
    "                \"playwright\": {\n",
    "                \"command\": \"npx\",  # Use resolved npx path\n",
    "                \"args\": [\n",
    "                    \"@playwright/mcp@latest\",\n",
    "                    \"--browser\", \"chrome\"  # Options: chrome, firefox, webkit, msedge\n",
    "                ],\n",
    "                }\n",
    "            }\n",
    "        self.tools = None\n",
    "        self.cleanup_func = None\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"42\"}}\n",
    "        self.graph = None\n",
    "        self.checkpointer = MemorySaver()\n",
    "        self.reasoning_agent = False\n",
    "        self.initialized = False\n",
    "        self.human_in_the_loop = False\n",
    "        \n",
    "        # Add default system prompt\n",
    "        self.default_system_prompt = \"You are a helpful browser assistant. Use the browser tools to help the user.\"\n",
    "        # Initialize prompt template with default\n",
    "        self.prompt_template = self.create_default_prompt_template()\n",
    "    \n",
    "    \n",
    "    def create_default_prompt_template(self):\n",
    "        \"\"\"Create a default prompt template that takes a query parameter\"\"\"\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        \n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.default_system_prompt),\n",
    "            (\"human\", \"{query}\")\n",
    "        ])\n",
    "        return template\n",
    "        \n",
    "    def set_prompt_template(self, template: ChatPromptTemplate):\n",
    "        \"\"\"Set a custom prompt template for the agent.\"\"\"\n",
    "        self.prompt_template = template\n",
    "        logger.info(\"Custom prompt template set for agent\")\n",
    "        \n",
    "    async def set_agent_type(self, reasoning: bool = False):\n",
    "        \"\"\"Set the agent type and reinitialize the graph if needed\"\"\"\n",
    "        if self.reasoning_agent == reasoning and self.initialized:\n",
    "            # No change needed\n",
    "            logger.info(f\"Agent already set to {'reasoning' if reasoning else 'standard'} type\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Changing agent type from {'reasoning' if self.reasoning_agent else 'standard'} to {'reasoning' if reasoning else 'standard'}\")\n",
    "        self.reasoning_agent = reasoning\n",
    "        \n",
    "        # Only rebuild graph if we've already initialized\n",
    "        if self.initialized and self.tools:\n",
    "            logger.info(f\"Rebuilding graph for {'reasoning' if reasoning else 'standard'} agent\")\n",
    "            \n",
    "            self.graph = create_agent_graph(\n",
    "                    self.tools,\n",
    "                    self.checkpointer\n",
    "                )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    async def initialize(self):\n",
    "        self.tools, self.cleanup_func = await convert_mcp_to_langchain_tools(self.mcp_servers)\n",
    "        \n",
    "        if self.tools:\n",
    "            print(\"MCP tools loaded successfully\")\n",
    "            self.initialized = True  # Set flag to True after successful initialization\n",
    "            self.graph = create_agent_graph(\n",
    "                    self.tools,\n",
    "                    self.checkpointer\n",
    "                )\n",
    "        else:\n",
    "            raise Exception(\"No tools were loaded from MCP servers\")\n",
    "        \n",
    "    async def astream_events(self, input):\n",
    "        \"\"\"Stream events from the agent with flexible input handling.\n",
    "        \n",
    "        Args:\n",
    "            input: Either a string query or a list of BaseMessages\n",
    "            config: Optional configuration dictionary (defaults to self.config)\n",
    "        \n",
    "        Yields:\n",
    "            Events from the agent execution\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            await self.initialize()\n",
    "        \n",
    "        # Handle string query by formatting with prompt template\n",
    "        if isinstance(input, str):\n",
    "            logger.info(\"Formatting query using prompt template\")\n",
    "            messages = self.prompt_template.format_messages(query=input)\n",
    "        else:\n",
    "            # Assume input is already a list of messages\n",
    "            logger.info(\"Using provided message list\")\n",
    "            messages = input\n",
    "            \n",
    "        # Create the initial state\n",
    "        initial_state = AgentState(\n",
    "            messages=messages,\n",
    "            model_name=self.llm.model_name,\n",
    "            human_in_the_loop=self.human_in_the_loop,\n",
    "            testing=False,\n",
    "            test_actions=[],\n",
    "            return_direct=False,\n",
    "            intermediate_steps=[],\n",
    "            DEBUG=True,\n",
    "            prompt_template=self.prompt_template  # Pass prompt template to state\n",
    "        )\n",
    "        \n",
    "        print(f\"Using {'reasoning' if self.reasoning_agent else 'standard'} agent graph\")\n",
    "        \n",
    "        # Stream events from the graph\n",
    "        async for event in self.graph.astream_events(\n",
    "            initial_state,\n",
    "            config=self.config,\n",
    "            stream_mode=\"values\"\n",
    "        ):\n",
    "            if event[\"event\"] == \"on_chat_model_stream\":\n",
    "                chunk = event[\"data\"][\"chunk\"]\n",
    "                chunk.content = await self.format_response(chunk.content)\n",
    "                yield event\n",
    "\n",
    "    async def format_response(self, content: str) -> str:\n",
    "        \"\"\"Format the response content. Can be overridden for custom formatting.\"\"\"\n",
    "        return content\n",
    "        \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up MCP servers and other resources.\"\"\"\n",
    "        if self.cleanup_func:\n",
    "            try:\n",
    "                await self.cleanup_func()\n",
    "                print(\"MCP tools cleanup completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during MCP tools cleanup: {e}\")\n",
    "        else:\n",
    "            print(\"No cleanup function available\")\n",
    "        # Reset initialized state\n",
    "        self.initialized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7300a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"mcpServers\": initializing with: {'playwright': {'command': 'npx', 'args': ['@playwright/mcp@latest', '--browser', 'chrome']}}\n",
      "ERROR:langchain_mcp_tools.langchain_mcp_tools:Error spawning MCP server: [Errno 13] Permission denied: ''\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run the agent with human-in-the-loop enabled\u001b[39;00m\n\u001b[32m     30\u001b[39m task = \u001b[33m\"\u001b[39m\u001b[33mGo to bing.com and search for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython programming language\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_agent(task, human_in_the_loop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(query, human_in_the_loop)\u001b[39m\n\u001b[32m      9\u001b[39m agent.human_in_the_loop = human_in_the_loop\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Initialize the agent (loads tools)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent.initialize()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Create input messages\u001b[39;00m\n\u001b[32m     15\u001b[39m system_message = SystemMessage(content=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful browser assistant. Use the browser tools to help the user.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mMCPAgent.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28mself\u001b[39m.tools, \u001b[38;5;28mself\u001b[39m.cleanup_func = \u001b[38;5;28;01mawait\u001b[39;00m convert_mcp_to_langchain_tools(\u001b[38;5;28mself\u001b[39m.mcp_servers)\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tools:\n\u001b[32m     90\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMCP tools loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py:374\u001b[39m, in \u001b[36mconvert_mcp_to_langchain_tools\u001b[39m\u001b[34m(server_configs, logger)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# Spawn all MCP servers concurrently\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m server_name, server_config \u001b[38;5;129;01min\u001b[39;00m server_configs.items():\n\u001b[32m    370\u001b[39m     \u001b[38;5;66;03m# NOTE: the following `await` only blocks until the server subprocess\u001b[39;00m\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# is spawned, i.e. after returning from the `await`, the spawned\u001b[39;00m\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# subprocess starts its initialization independently of (so in\u001b[39;00m\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# parallel with) the Python execution of the following lines.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     transport = \u001b[38;5;28;01mawait\u001b[39;00m spawn_mcp_server_and_get_transport(\n\u001b[32m    375\u001b[39m         server_name,\n\u001b[32m    376\u001b[39m         server_config,\n\u001b[32m    377\u001b[39m         async_exit_stack,\n\u001b[32m    378\u001b[39m         logger\n\u001b[32m    379\u001b[39m     )\n\u001b[32m    380\u001b[39m     transports.append(transport)\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# Convert tools from each server to LangChain format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py:152\u001b[39m, in \u001b[36mspawn_mcp_server_and_get_transport\u001b[39m\u001b[34m(server_name, server_config, exit_stack, logger)\u001b[39m\n\u001b[32m    150\u001b[39m         errlog_val = server_config.get(\u001b[33m'\u001b[39m\u001b[33merrlog\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    151\u001b[39m         kwargs = {\u001b[33m'\u001b[39m\u001b[33merrlog\u001b[39m\u001b[33m'\u001b[39m: errlog_val} \u001b[38;5;28;01mif\u001b[39;00m errlog_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         transport = \u001b[38;5;28;01mawait\u001b[39;00m exit_stack.enter_async_context(\n\u001b[32m    153\u001b[39m             stdio_client(server_parameters, **kwargs)\n\u001b[32m    154\u001b[39m         )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    156\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mError spawning MCP server: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/contextlib.py:650\u001b[39m, in \u001b[36mAsyncExitStack.enter_async_context\u001b[39m\u001b[34m(self, cm)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object does \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnot support the asynchronous context manager protocol\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m                    ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m _enter(cm)\n\u001b[32m    651\u001b[39m \u001b[38;5;28mself\u001b[39m._push_async_cm_exit(cm, _exit)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/mcp/client/stdio/__init__.py:113\u001b[39m, in \u001b[36mstdio_client\u001b[39m\u001b[34m(server, errlog)\u001b[39m\n\u001b[32m    110\u001b[39m command = _get_executable_command(server.command)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Open process with stderr piped for capture\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m process = \u001b[38;5;28;01mawait\u001b[39;00m _create_platform_compatible_process(\n\u001b[32m    114\u001b[39m     command=command,\n\u001b[32m    115\u001b[39m     args=server.args,\n\u001b[32m    116\u001b[39m     env=(\n\u001b[32m    117\u001b[39m         {**get_default_environment(), **server.env}\n\u001b[32m    118\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m server.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m get_default_environment()\n\u001b[32m    120\u001b[39m     ),\n\u001b[32m    121\u001b[39m     errlog=errlog,\n\u001b[32m    122\u001b[39m     cwd=server.cwd,\n\u001b[32m    123\u001b[39m )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstdout_reader\u001b[39m():\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m process.stdout, \u001b[33m\"\u001b[39m\u001b[33mOpened process is missing stdout\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/mcp/client/stdio/__init__.py:212\u001b[39m, in \u001b[36m_create_platform_compatible_process\u001b[39m\u001b[34m(command, args, env, errlog, cwd)\u001b[39m\n\u001b[32m    210\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m create_windows_process(command, args, env, errlog, cwd)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m anyio.open_process(\n\u001b[32m    213\u001b[39m         [command, *args], env=env, stderr=errlog, cwd=cwd\n\u001b[32m    214\u001b[39m     )\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m process\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/anyio/_core/_subprocesses.py:184\u001b[39m, in \u001b[36mopen_process\u001b[39m\u001b[34m(command, stdin, stdout, stderr, cwd, env, startupinfo, creationflags, start_new_session, pass_fds, user, group, extra_groups, umask)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m umask >= \u001b[32m0\u001b[39m:\n\u001b[32m    182\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mumask\u001b[39m\u001b[33m\"\u001b[39m] = umask\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().open_process(\n\u001b[32m    185\u001b[39m     command,\n\u001b[32m    186\u001b[39m     stdin=stdin,\n\u001b[32m    187\u001b[39m     stdout=stdout,\n\u001b[32m    188\u001b[39m     stderr=stderr,\n\u001b[32m    189\u001b[39m     cwd=cwd,\n\u001b[32m    190\u001b[39m     env=env,\n\u001b[32m    191\u001b[39m     startupinfo=startupinfo,\n\u001b[32m    192\u001b[39m     creationflags=creationflags,\n\u001b[32m    193\u001b[39m     start_new_session=start_new_session,\n\u001b[32m    194\u001b[39m     pass_fds=pass_fds,\n\u001b[32m    195\u001b[39m     **kwargs,\n\u001b[32m    196\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:2552\u001b[39m, in \u001b[36mAsyncIOBackend.open_process\u001b[39m\u001b[34m(cls, command, stdin, stdout, stderr, **kwargs)\u001b[39m\n\u001b[32m   2544\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_shell(\n\u001b[32m   2545\u001b[39m         command,\n\u001b[32m   2546\u001b[39m         stdin=stdin,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2549\u001b[39m         **kwargs,\n\u001b[32m   2550\u001b[39m     )\n\u001b[32m   2551\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2552\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m   2553\u001b[39m         *command,\n\u001b[32m   2554\u001b[39m         stdin=stdin,\n\u001b[32m   2555\u001b[39m         stdout=stdout,\n\u001b[32m   2556\u001b[39m         stderr=stderr,\n\u001b[32m   2557\u001b[39m         **kwargs,\n\u001b[32m   2558\u001b[39m     )\n\u001b[32m   2560\u001b[39m stdin_stream = StreamWriterWrapper(process.stdin) \u001b[38;5;28;01mif\u001b[39;00m process.stdin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2561\u001b[39m stdout_stream = StreamReaderWrapper(process.stdout) \u001b[38;5;28;01mif\u001b[39;00m process.stdout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/subprocess.py:223\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    220\u001b[39m loop = events.get_running_loop()\n\u001b[32m    221\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    222\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    224\u001b[39m     protocol_factory,\n\u001b[32m    225\u001b[39m     program, *args,\n\u001b[32m    226\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    227\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/base_events.py:1708\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1706\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1707\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1708\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1709\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1710\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1712\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/unix_events.py:207\u001b[39m, in \u001b[36m_UnixSelectorEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33masyncio.get_child_watcher() is not activated, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33msubprocess support is not installed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    206\u001b[39m waiter = \u001b[38;5;28mself\u001b[39m.create_future()\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m transp = \u001b[43m_UnixSubprocessTransport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mwaiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwaiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m watcher.add_child_handler(transp.get_pid(),\n\u001b[32m    213\u001b[39m                           \u001b[38;5;28mself\u001b[39m._child_watcher_callback, transp)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/base_subprocess.py:36\u001b[39m, in \u001b[36mBaseSubprocessTransport.__init__\u001b[39m\u001b[34m(self, loop, protocol, args, shell, stdin, stdout, stderr, bufsize, waiter, extra, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create the child process: set the _proc attribute\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/unix_events.py:818\u001b[39m, in \u001b[36m_UnixSubprocessTransport._start\u001b[39m\u001b[34m(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m     stdin, stdin_w = socket.socketpair()\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m        \u001b[49m\u001b[43muniversal_newlines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stdin_w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    822\u001b[39m         stdin.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1022\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1955\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[39m\n\u001b[32m   1953\u001b[39m     err_msg = os.strerror(errno_num)\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[32m   1956\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1957\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: ''"
     ]
    }
   ],
   "source": [
    "# Add a new cell to your notebook with this code\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "async def run_agent(query: str, human_in_the_loop: bool = False):\n",
    "    # Create the agent\n",
    "    agent = MCPAgent()\n",
    "    \n",
    "    # Set human-in-the-loop mode\n",
    "    agent.human_in_the_loop = human_in_the_loop\n",
    "    \n",
    "    # Initialize the agent (loads tools)\n",
    "    await agent.initialize()\n",
    "    \n",
    "    # Create input messages\n",
    "    system_message = SystemMessage(content=\"You are a helpful browser assistant. Use the browser tools to help the user.\")\n",
    "    human_message = HumanMessage(content=query)\n",
    "    messages = [system_message, human_message]\n",
    "    \n",
    "    # Stream events from the agent\n",
    "    print(f\"Running agent with query: {query} (Human-in-the-loop: {human_in_the_loop})\")\n",
    "    async for event in agent.astream_events(messages):\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "    # Clean up when done\n",
    "    await agent.cleanup()\n",
    "\n",
    "# Run the agent with human-in-the-loop enabled\n",
    "task = \"Go to bing.com and search for 'python programming language'\"\n",
    "await run_agent(task, human_in_the_loop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
