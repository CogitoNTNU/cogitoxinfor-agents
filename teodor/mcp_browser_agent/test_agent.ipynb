{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdac0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Union, Sequence, List, Any, Optional, Dict, Literal\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage, ToolMessage, AIMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from load_model import load_model\n",
    "import json\n",
    "import operator\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "import asyncio\n",
    "from langgraph.types import interrupt, Command\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"agent_graph\")\n",
    "\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the agent's predicted action and its arguments.\n",
    "    \"\"\"\n",
    "    action: str = Field(description=\"The action to be performed by the agent\")\n",
    "    args: List[str] = Field(default_factory=list, description=\"Arguments for the action\")\n",
    "\n",
    "\n",
    "class OldState(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the state of the agent during execution.\n",
    "    \"\"\"\n",
    "    testing: bool = Field(default=False, description=\"Flag indicating if the agent is in testing mode\")\n",
    "    test_actions: Optional[List[Prediction]] = Field(default_factory=list, description=\"The action to be tested\")\n",
    "    prediction: Optional[Prediction] = Field(None, description=\"The agent's predicted action and arguments\")\n",
    "    repeated_failures: Optional[int] = Field(default=0, description=\"Count of repeated failures for the current action\")\n",
    "    human_in_the_loop: bool = False\n",
    "    DEBUG: bool = True\n",
    "\n",
    "\n",
    "class AgentState(OldState):\n",
    "    \"\"\"State of the agent.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None] = None\n",
    "    return_direct: bool = False\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add] = Field(default_factory=list)\n",
    "    model_name: str = \"gpt-4.1\"  # Default model name\n",
    "    test_responses: Optional[List[Dict[str, Any]]] = None  # Predefined responses for testing\n",
    "    prompt_template: Optional[ChatPromptTemplate] = None  # Added prompt template field\n",
    "    interrupt_response: Optional[str] = None  # Added interrupt response field\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        # Add this to handle serialization more gracefully\n",
    "        json_encoders = {\n",
    "            ChatPromptTemplate: lambda v: None if v is None else str(type(v).__name__),\n",
    "            BaseMessage: lambda v: None if v is None else str(type(v).__name__),\n",
    "        }\n",
    "\n",
    "\n",
    "def create_agent_graph(tools, checkpointer=None):\n",
    "    \"\"\"Create an async LangGraph REACT agent with customizable prompts and testing capabilities.\n",
    "    \n",
    "    Args:\n",
    "        tools: List of available tools\n",
    "        checkpointer: Optional checkpointer for state persistence\n",
    "        \n",
    "    Returns:\n",
    "        Compiled async StateGraph for the agent\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating agent graph with {len(tools)} tools\")\n",
    "\n",
    "    @RunnableLambda\n",
    "    async def call_model(state: AgentState, config: RunnableConfig):\n",
    "        \"\"\"Call the LLM with the current conversation state.\"\"\"\n",
    "        logger.info(f\"Calling model {state.model_name} with {len(state.messages)} messages\")\n",
    "        \n",
    "        llm = load_model(\n",
    "            model_name=state.model_name, \n",
    "            tools=tools, \n",
    "            )\n",
    "            \n",
    "        response = await llm.ainvoke(input=state.messages, config=config)\n",
    "        # Return updated state with the response\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    @RunnableLambda\n",
    "    async def human_input(state: AgentState, config: RunnableConfig) -> Union[AgentState, dict]:\n",
    "        # Extract the tool call details\n",
    "        last_message = state.messages[-1]\n",
    "        tool_call = last_message.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        # Interrupt the graph and wait for human input\n",
    "        message = f\"Agent wants to perform: {tool_name} with args: {tool_args}\"\n",
    "        user_input = interrupt(message)\n",
    "        \n",
    "        # Different routing based on user input\n",
    "        if not user_input or user_input.strip().lower() == \"approved\":\n",
    "            # User approved - continue with tool execution\n",
    "            return Command(goto=\"tools\")\n",
    "        \n",
    "        elif user_input.strip().lower() == \"exit\":\n",
    "            # User wants to stop execution\n",
    "            return Command(goto=END)\n",
    "        \n",
    "        elif user_input.strip().lower() in [\"no\", \"cancel\", \"reject\"]:\n",
    "            # User rejected - send back to agent with request for alternative\n",
    "            return Command(\n",
    "                goto=\"agent\", \n",
    "                update={\"messages\": state.messages + [\n",
    "                    HumanMessage(content=\"Please suggest an alternative approach.\")\n",
    "                ]}\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # User provided custom instructions - send to agent with those instructions\n",
    "            return Command(\n",
    "                goto=\"agent\",\n",
    "                update={\"messages\": state.messages + [\n",
    "                    HumanMessage(content=user_input)\n",
    "                ]}\n",
    "            )\n",
    "\n",
    "    @RunnableLambda\n",
    "    async def process_tool_execution(state: AgentState):\n",
    "        \"\"\"Execute tools and track intermediate steps.\"\"\"\n",
    "        # Get the last message with tool calls\n",
    "        last_message = state.messages[-1]\n",
    "        if not (isinstance(last_message, AIMessage) and last_message.tool_calls):\n",
    "            return state\n",
    "\n",
    "        tool_call = last_message.tool_calls[0]\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_input = tool_call[\"args\"]\n",
    "        \n",
    "        # Log the tool execution\n",
    "        logger.info(f\"Executing tool: {tool_name} with input: {json.dumps(tool_input)[:100]}...\")\n",
    "        \n",
    "        # Create tool node for this execution\n",
    "        tool_node = ToolNode(\n",
    "            tools=tools,\n",
    "            handle_tool_errors=lambda exception, tool_call: (\n",
    "                f\"Error executing tool {tool_call.get('name')}: {str(exception)}\"\n",
    "            ),\n",
    "        )\n",
    "        tool_names = [tool.name for tool in tools]\n",
    "        if tool_name not in tool_names:\n",
    "            raise ValueError(f\"Tool {tool_name} not found in available tools: {tool_names}\")\n",
    "        result = await tool_node.ainvoke(state)\n",
    "        \n",
    "        # Create the agent action record\n",
    "        agent_action = AgentAction(\n",
    "            tool=tool_name,\n",
    "            tool_input=tool_input,\n",
    "            log=last_message.content\n",
    "        )\n",
    "        \n",
    "        # Get the tool message content\n",
    "        tool_message = result[\"messages\"][-1]\n",
    "        print(f\"Tool message content: {tool_message}\")\n",
    "        \n",
    "        # Add to intermediate steps\n",
    "        new_steps = state.intermediate_steps + [(agent_action, tool_message.content)]\n",
    "        \n",
    "        logger.info(f\"New intermediate steps: {len(new_steps)}\")\n",
    "        \n",
    "        # Return updated state\n",
    "        return {\n",
    "            \"messages\": result[\"messages\"],\n",
    "            \"intermediate_steps\": new_steps\n",
    "        }\n",
    "\n",
    "\n",
    "    # Create the state graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    logger.info(\"Initializing state graph\")\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"agent\", call_model)\n",
    "    workflow.add_node(\"tools\", process_tool_execution)\n",
    "    workflow.add_node(\"human_input_node\", human_input, destinations=(\"agent\", \"tools\", END))\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    \n",
    "    # Define conditional edge routing\n",
    "    def should_continue(state: AgentState) -> str:\n",
    "        \"\"\"Determine if we should continue with tools or end the conversation.\"\"\"\n",
    "        last_message = state.messages[-1]\n",
    "        \n",
    "        if state.human_in_the_loop and isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            logger.info(\"Decision: Human intervention required\")\n",
    "            return \"human_input_node\"\n",
    "        \n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            logger.info(\"Decision: Agent requested tool execution\")\n",
    "            return \"tools\"\n",
    "        else:\n",
    "            logger.info(\"Decision: Agent completed task, ending workflow\")\n",
    "            return \"end\"\n",
    "    \n",
    "    # Connect nodes\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"tools\": \"tools\",\n",
    "            \"end\": END,\n",
    "            \"human_input_node\": \"human_input_node\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    logger.info(\"Graph structure defined and edges connected\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    if checkpointer:\n",
    "        logger.info(\"Compiling graph with checkpointer\")\n",
    "        return workflow.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    logger.info(\"Compiling graph without checkpointer\")\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc384f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:agent_graph:Creating agent graph with 0 tools\n",
      "INFO:agent_graph:Initializing state graph\n",
      "INFO:agent_graph:Graph structure defined and edges connected\n",
      "INFO:agent_graph:Compiling graph with checkpointer\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "graph = create_agent_graph(tools=[], checkpointer=MemorySaver())\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional # Added Any, Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import logging # Add logging import\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ConfigLLM(BaseModel):\n",
    "    model_name: str = os.getenv(\"LLM_MODEL_NAME\", \"gpt-4.1-nano\")\n",
    "    temperature: float = 0.0\n",
    "\n",
    "class MCPAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ConfigLLM()\n",
    "        \n",
    "        self.mcp_servers = {\n",
    "                \"playwright\": {\n",
    "                \"command\": \"npx\",  # Use resolved npx path\n",
    "                \"args\": [\n",
    "                    \"@playwright/mcp@latest\",\n",
    "                    \"--browser\", \"chrome\"  # Options: chrome, firefox, webkit, msedge\n",
    "                ],\n",
    "                }\n",
    "            }\n",
    "        self.tools = None\n",
    "        self.cleanup_func = None\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"42\"}}\n",
    "        self.graph = None\n",
    "        self.checkpointer = MemorySaver()\n",
    "        self.reasoning_agent = False\n",
    "        self.initialized = False\n",
    "        self.human_in_the_loop = False\n",
    "        \n",
    "        # Add default system prompt\n",
    "        self.default_system_prompt = \"You are a helpful browser assistant. Use the browser tools to help the user.\"\n",
    "        # Initialize prompt template with default\n",
    "        self.prompt_template = self.create_default_prompt_template()\n",
    "    \n",
    "    \n",
    "    def create_default_prompt_template(self):\n",
    "        \"\"\"Create a default prompt template that takes a query parameter\"\"\"\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        \n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.default_system_prompt),\n",
    "            (\"human\", \"{query}\")\n",
    "        ])\n",
    "        return template\n",
    "        \n",
    "    def set_prompt_template(self, template: ChatPromptTemplate):\n",
    "        \"\"\"Set a custom prompt template for the agent.\"\"\"\n",
    "        self.prompt_template = template\n",
    "        logger.info(\"Custom prompt template set for agent\")\n",
    "        \n",
    "    async def set_agent_type(self, reasoning: bool = False):\n",
    "        \"\"\"Set the agent type and reinitialize the graph if needed\"\"\"\n",
    "        if self.reasoning_agent == reasoning and self.initialized:\n",
    "            # No change needed\n",
    "            logger.info(f\"Agent already set to {'reasoning' if reasoning else 'standard'} type\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Changing agent type from {'reasoning' if self.reasoning_agent else 'standard'} to {'reasoning' if reasoning else 'standard'}\")\n",
    "        self.reasoning_agent = reasoning\n",
    "        \n",
    "        # Only rebuild graph if we've already initialized\n",
    "        if self.initialized and self.tools:\n",
    "            logger.info(f\"Rebuilding graph for {'reasoning' if reasoning else 'standard'} agent\")\n",
    "            \n",
    "            self.graph = create_agent_graph(\n",
    "                    self.tools,\n",
    "                    self.checkpointer\n",
    "                )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    async def initialize(self):\n",
    "        self.tools, self.cleanup_func = await convert_mcp_to_langchain_tools(self.mcp_servers)\n",
    "        \n",
    "        if self.tools:\n",
    "            print(\"MCP tools loaded successfully\")\n",
    "            self.initialized = True  # Set flag to True after successful initialization\n",
    "            self.graph = create_agent_graph(\n",
    "                    self.tools,\n",
    "                    self.checkpointer\n",
    "                )\n",
    "        else:\n",
    "            raise Exception(\"No tools were loaded from MCP servers\")\n",
    "        \n",
    "    async def astream_events(self, input):\n",
    "        \"\"\"Stream events from the agent with flexible input handling.\n",
    "        \n",
    "        Args:\n",
    "            input: Either a string query or a list of BaseMessages\n",
    "            config: Optional configuration dictionary (defaults to self.config)\n",
    "        \n",
    "        Yields:\n",
    "            Events from the agent execution\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            await self.initialize()\n",
    "        \n",
    "        # Handle string query by formatting with prompt template\n",
    "        if isinstance(input, str):\n",
    "            logger.info(\"Formatting query using prompt template\")\n",
    "            messages = self.prompt_template.format_messages(query=input)\n",
    "        else:\n",
    "            # Assume input is already a list of messages\n",
    "            logger.info(\"Using provided message list\")\n",
    "            messages = input\n",
    "            \n",
    "        # Create the initial state\n",
    "        initial_state = AgentState(\n",
    "            messages=messages,\n",
    "            model_name=self.llm.model_name,\n",
    "            human_in_the_loop=self.human_in_the_loop,\n",
    "            testing=False,\n",
    "            test_actions=[],\n",
    "            return_direct=False,\n",
    "            intermediate_steps=[],\n",
    "            DEBUG=True,\n",
    "            prompt_template=self.prompt_template  # Pass prompt template to state\n",
    "        )\n",
    "        \n",
    "        print(f\"Using {'reasoning' if self.reasoning_agent else 'standard'} agent graph\")\n",
    "        \n",
    "        # Stream events from the graph\n",
    "        async for event in self.graph.astream_events(\n",
    "            initial_state,\n",
    "            config=self.config,\n",
    "            stream_mode=\"values\"\n",
    "        ):\n",
    "            if event[\"event\"] == \"on_chat_model_stream\":\n",
    "                chunk = event[\"data\"][\"chunk\"]\n",
    "                chunk.content = await self.format_response(chunk.content)\n",
    "                yield event\n",
    "\n",
    "    async def format_response(self, content: str) -> str:\n",
    "        \"\"\"Format the response content. Can be overridden for custom formatting.\"\"\"\n",
    "        return content\n",
    "        \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up MCP servers and other resources.\"\"\"\n",
    "        if self.cleanup_func:\n",
    "            try:\n",
    "                await self.cleanup_func()\n",
    "                print(\"MCP tools cleanup completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during MCP tools cleanup: {e}\")\n",
    "        else:\n",
    "            print(\"No cleanup function available\")\n",
    "        # Reset initialized state\n",
    "        self.initialized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7300a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_mcp_tools.langchain_mcp_tools:MCP server \"mcpServers\": initializing with: {'playwright': {'command': 'npx', 'args': ['@playwright/mcp@latest', '--browser', 'chrome']}}\n",
      "ERROR:langchain_mcp_tools.langchain_mcp_tools:Error spawning MCP server: [Errno 13] Permission denied: ''\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run the agent with human-in-the-loop enabled\u001b[39;00m\n\u001b[32m     30\u001b[39m task = \u001b[33m\"\u001b[39m\u001b[33mGo to bing.com and search for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython programming language\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_agent(task, human_in_the_loop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(query, human_in_the_loop)\u001b[39m\n\u001b[32m      9\u001b[39m agent.human_in_the_loop = human_in_the_loop\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Initialize the agent (loads tools)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent.initialize()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Create input messages\u001b[39;00m\n\u001b[32m     15\u001b[39m system_message = SystemMessage(content=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful browser assistant. Use the browser tools to help the user.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mMCPAgent.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28mself\u001b[39m.tools, \u001b[38;5;28mself\u001b[39m.cleanup_func = \u001b[38;5;28;01mawait\u001b[39;00m convert_mcp_to_langchain_tools(\u001b[38;5;28mself\u001b[39m.mcp_servers)\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tools:\n\u001b[32m     90\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMCP tools loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py:374\u001b[39m, in \u001b[36mconvert_mcp_to_langchain_tools\u001b[39m\u001b[34m(server_configs, logger)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# Spawn all MCP servers concurrently\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m server_name, server_config \u001b[38;5;129;01min\u001b[39;00m server_configs.items():\n\u001b[32m    370\u001b[39m     \u001b[38;5;66;03m# NOTE: the following `await` only blocks until the server subprocess\u001b[39;00m\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# is spawned, i.e. after returning from the `await`, the spawned\u001b[39;00m\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# subprocess starts its initialization independently of (so in\u001b[39;00m\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# parallel with) the Python execution of the following lines.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     transport = \u001b[38;5;28;01mawait\u001b[39;00m spawn_mcp_server_and_get_transport(\n\u001b[32m    375\u001b[39m         server_name,\n\u001b[32m    376\u001b[39m         server_config,\n\u001b[32m    377\u001b[39m         async_exit_stack,\n\u001b[32m    378\u001b[39m         logger\n\u001b[32m    379\u001b[39m     )\n\u001b[32m    380\u001b[39m     transports.append(transport)\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# Convert tools from each server to LangChain format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py:152\u001b[39m, in \u001b[36mspawn_mcp_server_and_get_transport\u001b[39m\u001b[34m(server_name, server_config, exit_stack, logger)\u001b[39m\n\u001b[32m    150\u001b[39m         errlog_val = server_config.get(\u001b[33m'\u001b[39m\u001b[33merrlog\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    151\u001b[39m         kwargs = {\u001b[33m'\u001b[39m\u001b[33merrlog\u001b[39m\u001b[33m'\u001b[39m: errlog_val} \u001b[38;5;28;01mif\u001b[39;00m errlog_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         transport = \u001b[38;5;28;01mawait\u001b[39;00m exit_stack.enter_async_context(\n\u001b[32m    153\u001b[39m             stdio_client(server_parameters, **kwargs)\n\u001b[32m    154\u001b[39m         )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    156\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mError spawning MCP server: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/contextlib.py:650\u001b[39m, in \u001b[36mAsyncExitStack.enter_async_context\u001b[39m\u001b[34m(self, cm)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object does \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnot support the asynchronous context manager protocol\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m                    ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m _enter(cm)\n\u001b[32m    651\u001b[39m \u001b[38;5;28mself\u001b[39m._push_async_cm_exit(cm, _exit)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/mcp/client/stdio/__init__.py:113\u001b[39m, in \u001b[36mstdio_client\u001b[39m\u001b[34m(server, errlog)\u001b[39m\n\u001b[32m    110\u001b[39m command = _get_executable_command(server.command)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Open process with stderr piped for capture\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m process = \u001b[38;5;28;01mawait\u001b[39;00m _create_platform_compatible_process(\n\u001b[32m    114\u001b[39m     command=command,\n\u001b[32m    115\u001b[39m     args=server.args,\n\u001b[32m    116\u001b[39m     env=(\n\u001b[32m    117\u001b[39m         {**get_default_environment(), **server.env}\n\u001b[32m    118\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m server.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m get_default_environment()\n\u001b[32m    120\u001b[39m     ),\n\u001b[32m    121\u001b[39m     errlog=errlog,\n\u001b[32m    122\u001b[39m     cwd=server.cwd,\n\u001b[32m    123\u001b[39m )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstdout_reader\u001b[39m():\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m process.stdout, \u001b[33m\"\u001b[39m\u001b[33mOpened process is missing stdout\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/mcp/client/stdio/__init__.py:212\u001b[39m, in \u001b[36m_create_platform_compatible_process\u001b[39m\u001b[34m(command, args, env, errlog, cwd)\u001b[39m\n\u001b[32m    210\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m create_windows_process(command, args, env, errlog, cwd)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m anyio.open_process(\n\u001b[32m    213\u001b[39m         [command, *args], env=env, stderr=errlog, cwd=cwd\n\u001b[32m    214\u001b[39m     )\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m process\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/anyio/_core/_subprocesses.py:184\u001b[39m, in \u001b[36mopen_process\u001b[39m\u001b[34m(command, stdin, stdout, stderr, cwd, env, startupinfo, creationflags, start_new_session, pass_fds, user, group, extra_groups, umask)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m umask >= \u001b[32m0\u001b[39m:\n\u001b[32m    182\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mumask\u001b[39m\u001b[33m\"\u001b[39m] = umask\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().open_process(\n\u001b[32m    185\u001b[39m     command,\n\u001b[32m    186\u001b[39m     stdin=stdin,\n\u001b[32m    187\u001b[39m     stdout=stdout,\n\u001b[32m    188\u001b[39m     stderr=stderr,\n\u001b[32m    189\u001b[39m     cwd=cwd,\n\u001b[32m    190\u001b[39m     env=env,\n\u001b[32m    191\u001b[39m     startupinfo=startupinfo,\n\u001b[32m    192\u001b[39m     creationflags=creationflags,\n\u001b[32m    193\u001b[39m     start_new_session=start_new_session,\n\u001b[32m    194\u001b[39m     pass_fds=pass_fds,\n\u001b[32m    195\u001b[39m     **kwargs,\n\u001b[32m    196\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cogitoxinfor-agents/.myvenv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:2552\u001b[39m, in \u001b[36mAsyncIOBackend.open_process\u001b[39m\u001b[34m(cls, command, stdin, stdout, stderr, **kwargs)\u001b[39m\n\u001b[32m   2544\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_shell(\n\u001b[32m   2545\u001b[39m         command,\n\u001b[32m   2546\u001b[39m         stdin=stdin,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2549\u001b[39m         **kwargs,\n\u001b[32m   2550\u001b[39m     )\n\u001b[32m   2551\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2552\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m   2553\u001b[39m         *command,\n\u001b[32m   2554\u001b[39m         stdin=stdin,\n\u001b[32m   2555\u001b[39m         stdout=stdout,\n\u001b[32m   2556\u001b[39m         stderr=stderr,\n\u001b[32m   2557\u001b[39m         **kwargs,\n\u001b[32m   2558\u001b[39m     )\n\u001b[32m   2560\u001b[39m stdin_stream = StreamWriterWrapper(process.stdin) \u001b[38;5;28;01mif\u001b[39;00m process.stdin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2561\u001b[39m stdout_stream = StreamReaderWrapper(process.stdout) \u001b[38;5;28;01mif\u001b[39;00m process.stdout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/subprocess.py:223\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    220\u001b[39m loop = events.get_running_loop()\n\u001b[32m    221\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    222\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    224\u001b[39m     protocol_factory,\n\u001b[32m    225\u001b[39m     program, *args,\n\u001b[32m    226\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    227\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/base_events.py:1708\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1706\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1707\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1708\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1709\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1710\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1712\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/unix_events.py:207\u001b[39m, in \u001b[36m_UnixSelectorEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33masyncio.get_child_watcher() is not activated, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33msubprocess support is not installed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    206\u001b[39m waiter = \u001b[38;5;28mself\u001b[39m.create_future()\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m transp = \u001b[43m_UnixSubprocessTransport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mwaiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwaiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m watcher.add_child_handler(transp.get_pid(),\n\u001b[32m    213\u001b[39m                           \u001b[38;5;28mself\u001b[39m._child_watcher_callback, transp)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/base_subprocess.py:36\u001b[39m, in \u001b[36mBaseSubprocessTransport.__init__\u001b[39m\u001b[34m(self, loop, protocol, args, shell, stdin, stdout, stderr, bufsize, waiter, extra, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create the child process: set the _proc attribute\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/unix_events.py:818\u001b[39m, in \u001b[36m_UnixSubprocessTransport._start\u001b[39m\u001b[34m(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m     stdin, stdin_w = socket.socketpair()\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m        \u001b[49m\u001b[43muniversal_newlines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stdin_w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    822\u001b[39m         stdin.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1022\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1955\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[39m\n\u001b[32m   1953\u001b[39m     err_msg = os.strerror(errno_num)\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[32m   1956\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1957\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: ''"
     ]
    }
   ],
   "source": [
    "# Add a new cell to your notebook with this code\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "async def run_agent(query: str, human_in_the_loop: bool = False):\n",
    "    # Create the agent\n",
    "    agent = MCPAgent()\n",
    "    \n",
    "    # Set human-in-the-loop mode\n",
    "    agent.human_in_the_loop = human_in_the_loop\n",
    "    \n",
    "    # Initialize the agent (loads tools)\n",
    "    await agent.initialize()\n",
    "    \n",
    "    # Create input messages\n",
    "    system_message = SystemMessage(content=\"You are a helpful browser assistant. Use the browser tools to help the user.\")\n",
    "    human_message = HumanMessage(content=query)\n",
    "    messages = [system_message, human_message]\n",
    "    \n",
    "    # Stream events from the agent\n",
    "    print(f\"Running agent with query: {query} (Human-in-the-loop: {human_in_the_loop})\")\n",
    "    async for event in agent.astream_events(messages):\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "    # Clean up when done\n",
    "    await agent.cleanup()\n",
    "\n",
    "# Run the agent with human-in-the-loop enabled\n",
    "task = \"Go to bing.com and search for 'python programming language'\"\n",
    "await run_agent(task, human_in_the_loop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
